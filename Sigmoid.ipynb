

import numpy as np

def sigmoid_function(x):
    return 1 / (1 + np.exp(-x))

def derivative(x):
    return x * (1 - x)

# Take inputs
x1 = 0.4
x2 = 0.8

# Actual output
y_actual = 1

# Take weight and biases
w1, w2, w3, b1, b2 = np.random.rand(5)

learning_rate = 0.1
epochs = 100
losses = []

for epoch in range(epochs):
    # Forward propagation
    # For hidden layer
    z_hid = w1 * x1 + w2 * x2 + b1
    h = sigmoid_function(z_hid)

    # For output layer
    z_out = w3 * h + b2
    y_pred = sigmoid_function(z_out)

    # Error calculation
    E = (1 / 2) * (y_actual - y_pred) ** 2
    losses.append(E)

    # Backpropagation
    dE_dypred = -(y_actual - y_pred)
    dypred_dh = derivative(y_pred) * w3
    dh_dw1 = derivative(h) * x1
    dh_dw2 = derivative(h) * x2

    # Gradients calculation
    dw1 = dE_dypred * dypred_dh * dh_dw1
    dw2 = dE_dypred * dypred_dh * dh_dw2
    dw3 = dE_dypred * derivative(y_pred) * h
    db1 = dE_dypred * dypred_dh * derivative(h)
    db2 = dE_dypred * derivative(y_pred)

    # Update weight and bias
    w1 = w1 - learning_rate * dw1
    w2 = w2 - learning_rate * dw2
    w3 = w3 - learning_rate * dw3
    b1 = b1 - learning_rate * db1
    b2 = b2 - learning_rate * db2

    if epoch % 10 == 0:
        print(f"Epoch: {epoch}, Loss: {E}")
